{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn as sk\n",
    "from datetime import date\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 date   pitcher    batter    outcome\n",
      "count                         2695558   2695558   2695558    2695558\n",
      "unique                            NaN      2721      3730         11\n",
      "top                               NaN  schem001  freef001  strikeout\n",
      "freq                              NaN     12342      9310     487899\n",
      "mean    2017-05-09 02:17:18.417723136       NaN       NaN        NaN\n",
      "min               2011-03-31 00:00:00       NaN       NaN        NaN\n",
      "25%               2014-04-18 00:00:00       NaN       NaN        NaN\n",
      "50%               2017-05-09 00:00:00       NaN       NaN        NaN\n",
      "75%               2020-09-08 00:00:00       NaN       NaN        NaN\n",
      "max               2023-10-01 00:00:00       NaN       NaN        NaN\n"
     ]
    }
   ],
   "source": [
    "# import the data\n",
    "path = '/home/projects/baseball-datasets/data/final/atbats.csv'\n",
    "df = pd.read_csv(path, parse_dates=['date'])\n",
    "df = df[['date', 'pitcher', 'batter', 'outcome']]\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown outcomes:  358072\n",
      "Number of at bats:  2695558\n",
      "Number of pitchers:  2721\n",
      "Number of batters:  3730\n",
      "Number of seasons:  2011  to  2023\n",
      "Date format:  <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "# Do some exploratory data analysis\n",
    "# how many outcomes with unknown values?\n",
    "print('Number of unknown outcomes: ', len(df[df.outcome == 'unknown']))\n",
    "# How many at bats are there?\n",
    "print('Number of at bats: ', len(df))\n",
    "# How many unique pitchers are there?\n",
    "print('Number of pitchers: ', len(df.pitcher.unique()))\n",
    "# How many unique batters are there?\n",
    "print('Number of batters: ', len(df.batter.unique()))\n",
    "# spanning how many seasons?\n",
    "print('Number of seasons: ', min(df.date.dt.year), ' to ', max(df.date.dt.year))\n",
    "# check to make sure date is in the right format\n",
    "print('Date format: ', type(df.date[0]))\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "# onehot encode the outcome into multiple columns as 0 or 1\n",
    "\n",
    "mean_df = pd.get_dummies(df, columns=['outcome'], dtype='int64')\n",
    "# df.head()\n",
    "# convert datatype to in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['popout', 'walk', 'double', 'single', 'flyout', 'groundout',\n",
       "       'lineout', 'strikeout', 'homerun', 'unknown', 'triple'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function that splits the data into train, test, and validation sets, the year of 2023 is used as the test set\n",
    "def split_data(df, train_cuttoff='2022-01-01', test_cuttoff='2023-01-01'):\n",
    "    train = df[df['date'] < train_cuttoff]\n",
    "    valid = df[(df['date'] >= train_cuttoff) & (df['date'] < test_cuttoff)]\n",
    "    test = df[df['date'] >= test_cuttoff]\n",
    "    return train, valid, test\n",
    "\n",
    "train_df, valid_df, test_df = split_data(df)\n",
    "train_df.head()\n",
    "train_df['outcome'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0775, 0.1076, 0.1496, 0.0369, 0.0415, 0.0365, 0.1568, 0.1789, 0.0068,\n",
      "        0.1337, 0.0740], dtype=torch.float64)\n",
      "Test loss:  2.3628871009774945\n",
      "Valid loss:  2.362509602861311\n"
     ]
    }
   ],
   "source": [
    "# Get the baseline accuracy for the test set based on predicting a distribution of the training set\n",
    "# Convert the train, test, and valid set to a tensor\n",
    "train_df_mean, valid_df_mean, test_df_mean = split_data(mean_df)\n",
    "train_tensor = torch.tensor(train_df_mean.iloc[:, 3:].to_numpy().astype(float), dtype=torch.float64)\n",
    "test_tensor = torch.tensor(test_df_mean.iloc[:, 3:].to_numpy().astype(float), dtype=torch.float64)\n",
    "valid_tensor = torch.tensor(valid_df_mean.iloc[:, 3:].to_numpy().astype(float), dtype=torch.float64)\n",
    "\n",
    "# Get the distribution of the training set\n",
    "train_dist = torch.mean(train_tensor, dim=0, dtype=torch.float64)\n",
    "# duplicate for every row in the test set\n",
    "train_dist_test = train_dist.repeat(len(test_tensor), 1)\n",
    "# duplicate for every row in the valid set\n",
    "train_dist_valid = train_dist.repeat(len(valid_tensor), 1)\n",
    "\n",
    "# Calculate the cross entropy loss of the test set\n",
    "valid_loss = torch.nn.functional.cross_entropy(train_dist_test, test_tensor)\n",
    "test_loss = torch.nn.functional.cross_entropy(train_dist_valid, valid_tensor)\n",
    "print(train_dist)\n",
    "print('Test loss: ', test_loss.item())\n",
    "print('Valid loss: ', valid_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use a Neural Matrix Factorization Model in order to try and predict the outcomes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_pitchers, num_batters, num_outcomes, embed_dim=50, num_layers=2\n",
    "    ):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        # Increase the embedding dimension\n",
    "        self.pitcher_embedding = nn.Embedding(num_pitchers, embed_dim)\n",
    "        self.batter_embedding = nn.Embedding(num_batters, embed_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(embed_dim * 2, 128)  # Increase the hidden layer size\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(128, 128) for _ in range(num_layers - 2)]\n",
    "        )\n",
    "        self.dropout = nn.ModuleList([nn.Dropout(0.3) for _ in range(num_layers - 1)])\n",
    "        self.output_layer = nn.Linear(128, num_outcomes)\n",
    "\n",
    "        # Add Dropout layers\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, pitcher_ids, batter_ids):\n",
    "        pitcher_embed = self.pitcher_embedding(pitcher_ids)\n",
    "        batter_embed = self.batter_embedding(batter_ids)\n",
    "        x = torch.cat([pitcher_embed, batter_embed], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout[0](x)  # Dropout layer after first hidden layer\n",
    "        for idx in range(len(self.linear_layers)):\n",
    "            x = F.relu(self.linear_layers[idx](x))\n",
    "            x = self.dropout[idx + 1](x)\n",
    "        \n",
    "        out = self.output_layer(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseballDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.pitcher = torch.tensor(df['pitcher'].to_numpy().astype(int), dtype=torch.long)\n",
    "        self.batter = torch.tensor(df['batter'].to_numpy().astype(int), dtype=torch.long)\n",
    "        self.data = torch.tensor(df['outcome'].to_numpy().astype(int), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming the first two columns are batter and pitcher indices, and the rest are outcomes\n",
    "        return self.pitcher[idx], self.batter[idx], self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, train_df, proportion=.15):\n",
    "        self.train_df = train_df\n",
    "        self.known_batter_ids = set(train_df['batter'])\n",
    "        self.known_pitcher_ids = set(train_df['pitcher'])\n",
    "\n",
    "        # Create mappings for batter and pitcher IDs to indices\n",
    "        self.batter_id_to_index = {id: idx for idx, id in enumerate(self.known_batter_ids, start=1)}  # Start from 1 to reserve 0 for 'unknown'\n",
    "        self.pitcher_id_to_index = {id: idx for idx, id in enumerate(self.known_pitcher_ids, start=1)}  # Start from 1 to reserve 0 for 'unknown' \n",
    "        self.batter_id_to_index['unknown'] = 0\n",
    "        self.pitcher_id_to_index['unknown'] = 0\n",
    "        self.outcomes = {\n",
    "            'single': 0, \n",
    "            'double': 1,\n",
    "            'triple': 2,\n",
    "            'homerun': 3,\n",
    "            'walk': 4,\n",
    "            'strikeout': 5,\n",
    "            'groundout': 6,\n",
    "            'flyout': 7,\n",
    "            'lineout':8,\n",
    "            'popout': 9,\n",
    "            'unknown': 10\n",
    "            }\n",
    "    \n",
    "    def augment(self, train_df, proportion=.15):\n",
    "        # sample the data according to the proportion and remove either the pitcher or batter\n",
    "        train_aug_pitcher = train_df.sample(frac=proportion/2)\n",
    "        train_aug_pitcher['pitcher'] = 'unknown'\n",
    "        train_aug_batter = train_df.sample(frac=proportion/2)\n",
    "        train_aug_batter['batter'] = 'unknown'\n",
    "        train_aug = pd.concat([train_df, train_aug_pitcher, train_aug_batter])\n",
    "        return train_aug\n",
    "\n",
    "    def preprocess(self, df, train=False, proportion=.15):\n",
    "        # Replace unknown IDs and convert to indices\n",
    "        processed_df = df.copy()\n",
    "        if train:\n",
    "            processed_df = self.augment(df, proportion=proportion)\n",
    "\n",
    "        \n",
    "        processed_df.loc[:, 'batter'] = processed_df['batter'].apply(lambda x: self.batter_id_to_index.get(x, 0))\n",
    "        processed_df.loc[:, 'pitcher'] = processed_df['pitcher'].apply(lambda x: self.pitcher_id_to_index.get(x, 0))\n",
    "        processed_df.loc[:, 'outcome'] = processed_df['outcome'].apply(lambda x: self.outcomes.get(x, 10))\n",
    "        # Also need to drop the date column\n",
    "        processed_df = processed_df.drop(columns=['date'])\n",
    "\n",
    "        return processed_df\n",
    "\n",
    "    def get_num_batters(self):\n",
    "        return len(self.batter_id_to_index)\n",
    "    \n",
    "    def get_num_pitchers(self):\n",
    "        return len(self.pitcher_id_to_index)    \n",
    "    \n",
    "    def get_num_outcomes(self):\n",
    "        return len(self.outcomes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_model(train_dataloader, test_dataloader, preprocessor, learning_rate, epochs, embed_dim, num_layers, early_stopping_patience=5):\n",
    "    # Initialize the model, loss, and optimizer with hyperparameters\n",
    "    num_pitchers = preprocessor.get_num_pitchers()\n",
    "    num_batters = preprocessor.get_num_batters()\n",
    "    num_outcomes = preprocessor.get_num_outcomes()\n",
    "    \n",
    "    model = NCF(num_pitchers, num_batters, num_outcomes, embed_dim=embed_dim, num_layers=num_layers)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('using: ', device)\n",
    "    # device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for i, (pitchers, batters, outcomes) in enumerate(train_dataloader):\n",
    "            pitchers, batters, outcomes = pitchers.to(device), batters.to(device), outcomes.to(device)\n",
    "            # check all data types\n",
    "            outputs = model(pitchers, batters)\n",
    "            loss = criterion(input=outputs, target=outcomes)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            print(loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for pitchers, batters, outcomes in test_dataloader:\n",
    "                pitchers, batters, outcomes = pitchers.to(device), batters.to(device), outcomes.to(device)\n",
    "                outputs = model(pitchers, batters)\n",
    "                loss = criterion(outputs, outcomes)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(test_dataloader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == early_stopping_patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    return val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using:  cuda:0\n",
      "Epoch [1/20], Train Loss: 2.1609, Validation Loss: 2.0800\n",
      "Epoch [2/20], Train Loss: 2.1303, Validation Loss: 2.0618\n",
      "Epoch [3/20], Train Loss: 2.1238, Validation Loss: 2.0485\n",
      "Epoch [4/20], Train Loss: 2.1207, Validation Loss: 2.0426\n",
      "Epoch [5/20], Train Loss: 2.1191, Validation Loss: 2.0455\n",
      "Epoch [6/20], Train Loss: 2.1180, Validation Loss: 2.0440\n",
      "Epoch [7/20], Train Loss: 2.1171, Validation Loss: 2.0462\n",
      "Epoch [8/20], Train Loss: 2.1163, Validation Loss: 2.0447\n",
      "Epoch [9/20], Train Loss: 2.1154, Validation Loss: 2.0417\n",
      "Epoch [10/20], Train Loss: 2.1150, Validation Loss: 2.0408\n",
      "Epoch [11/20], Train Loss: 2.1144, Validation Loss: 2.0436\n",
      "Epoch [12/20], Train Loss: 2.1138, Validation Loss: 2.0430\n",
      "Epoch [13/20], Train Loss: 2.1134, Validation Loss: 2.0435\n",
      "Epoch [14/20], Train Loss: 2.1130, Validation Loss: 2.0468\n",
      "Epoch [15/20], Train Loss: 2.1124, Validation Loss: 2.0478\n",
      "Early stopping triggered after 15 epochs\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "EMBED_DIM = 50\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "preprocessor = DataPreprocessor(train_df)\n",
    "\n",
    "train_df_processed = preprocessor.preprocess(train_df, train=True)\n",
    "valid_df_processed = preprocessor.preprocess(valid_df)\n",
    "test_df_processed = preprocessor.preprocess(test_df)\n",
    "\n",
    "# Create Dataset Instances\n",
    "train_dataset = BaseballDataset(train_df_processed)\n",
    "valid_dataset = BaseballDataset(valid_df_processed)\n",
    "test_dataset = BaseballDataset(test_df_processed)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "val_loss = train_model(train_dataloader, valid_dataloader, preprocessor, LEARNING_RATE, EPOCHS, EMBED_DIM, NUM_LAYERS)\n",
    "# print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_pitchers, num_batters, num_outcomes, embed_dim=50):\n",
    "        super(NCF, self).__init__()\n",
    "        # Increase the embedding dimension\n",
    "        self.pitcher_embedding = nn.Embedding(num_pitchers, embed_dim)\n",
    "        self.batter_embedding = nn.Embedding(num_batters, embed_dim)\n",
    "        self.output_layer = nn.Linear(embed_dim*2, num_outcomes)\n",
    "        # Add Dropout layers\n",
    "\n",
    "    def forward(self, pitcher_ids, batter_ids):\n",
    "        pitcher_embed = self.pitcher_embedding(pitcher_ids)\n",
    "        batter_embed = self.batter_embedding(batter_ids)\n",
    "        x = torch.cat([pitcher_embed, batter_embed], dim=1)\n",
    "        out = self.output_layer(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "pitcher = torch.tensor([1], dtype=torch.long).to(device)\n",
    "batter = torch.tensor([0], dtype=torch.long).to(device)\n",
    "\n",
    "model = NCF(2, 2, 3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "model.train()\n",
    "out = model(pitcher, batter)\n",
    "\n",
    "target = torch.tensor([1], dtype=torch.long)\n",
    "loss = criterion(out, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseball",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
